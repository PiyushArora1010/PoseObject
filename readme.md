# Task: Editing Pose of an Object in the Scene

## Overview
This README provides detailed steps, instructions, and observations for editing the pose of an object in the scene using segmentation, inpainting, and novel view generation techniques.

## How to Run
Once in the PoseObject folder,
1. Task 1: Execute the following command: python run.py -i <path to image> -c <user prompt for class> -tk 1
2. Task 2: Execute the following command: python run.py -po <polar angle> -az <azimuth angle> -i <path to image> -c <user prompt for class>

## Steps Followed for the Assignment

### Segmentation:
1. Segmentation was performed using the "Segment Anything" (SAM) model "sam_vit_h_4b8939.pth" checkpoint was used for segmentation. (Needs to be downloaded and pasted in the checkpoints folder)
2. Multiple segmentation masks were generated by the SAM model.
3. Comparison between text prompts and segmented images helped select the mask with maximum similarity.
4. Features of the text prompt and segmented images were extracted using the Clip vit-b/32 text and image feature extractor, respectively.

### Inpainting:
1. After segmentation, the mask along with the image was processed through the Stable Diffusion Inpainting pipeline.
2. The user prompt for inpainting was set as negative to guide the process.
3. The Runaway Stable Diffusion model was employed for inpainting tasks.

### Novel View Generation:
1. The zero-123-xl checkpoint was used for generating novel views of the image. (It needs to be downloaded and pasted in the nvs folder)
2. Novel views were generated based on the polar angle, azimuth angle, and the segmented object's image. A mask was then created for this novel image following the segmentation procedure mentioned above.
3. Objects were shifted towards the center of the image before generating novel views to avoid unexpected results.
4. After novel view generation, objects were moved back to their original positions using the bounding boxes obtained from the SAM model.

### Additional Processing:
1. Masks were dilated and eroded to enhance accuracy.
2. Thresholding was applied over the masks to refine segmentation results.

### Observations:
1. Results were highly seed-dependent.
2. The inpainting pipeline occasionally generated unexpected results, like inpainting random objects despite using negative prompts.
3. Novel view synthesis exhibited instability, with some results being nonsensical.
4. The pipeline performed well for objects like chairs and sofas but struggled with items such as laptops and lamps.
5. Larger angles posed challenges for the pipeline.
6. Segmentation masks were not always accurate, leading to incorrect inpainting and novel view generation results.

## Experiments:
1. Tested multiple repositories like TripoSR and One-2-3-45, with the One-2-3-45 repo providing the best results using the Zero123-xl checkpoint for Novel View Synthesis.
2. Initially used Segformer for segmentation but found SAM model masks more reliable.
3. Significant time was consumed due to GPU constraints.
4. The inpainting pipeline lacked consistency, with results varying across runs and seeds. Different prompts were tested to improve results, but the pipeline remained unpredictable.

## Ways to Improve
1. Employ a more robust inpainting pipeline to enhance consistency.
2. Use Clip Text feaures for prompt to SAM model to improve segmentation results.
3. Experiment with different kernel sizes for dilation and erosion to refine segmentation masks."# PoseObject" 
